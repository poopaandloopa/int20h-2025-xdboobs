networks:
  network:
    driver: bridge


volumes:
  mlflow_data:
  prefect_data:


services:
  # experiment tracking platform
  mlflow-server:
    build: ./mlflow
    networks: ['network']
    volumes:
      - mlflow_data:/mlflow
    env_file:
      - .env
    ports:
      - 5000:5000
    command: ["mlflow", "server",
              "--host", "0.0.0.0",
              "--port", "5000",
              "--default-artifact-root", "${MLFLOW_ARTIFACT_ROOT}"]
    restart: unless-stopped

  # pipeline orchestration server
#  prefect-server:
#    image: prefecthq/prefect:3-latest
#    networks: ['network']
#    volumes:
#      - prefect_data:/root/.prefect
#    ports:
#      - 4200:4200
#    command: ["prefect", "server", "start", "--host", "0.0.0.0"]
#    restart: unless-stopped

  # worker that executes pipelines
#  prefect-worker-1:
#    image: prefecthq/prefect:3-latest
#    env_file:
#      - .env
#    depends_on:
#      - prefect-server
#    command: ["prefect", "worker", "start", "--pool", "default-agent-pool"]
#    restart: unless-stopped

  # register pipeline in prefect
#  pipelines:
#    build: ./pipelines
#    networks: ['network']
#    env_file:
#      - .env
#    depends_on:
#      - prefect-server
#      - mlflow-server

#  inference-api:
#    build: ./inference_api
#    networks: ['network']
#    env_file:
#      - .env
#    ports:
#      - 8000:8000
#    depends_on:
#      - pipelines
#    restart: unless-stopped
#
#  streamlit-ui:
#    build: ./ui
#    networks: ['network']
#    env_file:
#      - .env
#    ports:
#      - 8501:8501
#    depends_on:
#      - inference-api
#    restart: unless-stopped
